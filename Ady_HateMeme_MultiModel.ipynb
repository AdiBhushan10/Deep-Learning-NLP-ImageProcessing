{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Ady_HateMeme_MultiModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6caf4a0ec3b8494a961b3dbf8a738ea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_112642ddaef74be18e732830d79af247",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f3fa88bd8335417bb1423f5158473f22",
              "IPY_MODEL_46f3761f2b5e42d3bdd44deed4259ab8"
            ]
          }
        },
        "112642ddaef74be18e732830d79af247": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f3fa88bd8335417bb1423f5158473f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_09e956fdf3e740eebd59561dd88449a9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_36fdc7d1d660471db60e8f2566f84526"
          }
        },
        "46f3761f2b5e42d3bdd44deed4259ab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1caeb0e0d5e44e0b905b587501de578a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:11&lt;00:00, 37.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_20125814f4a14c2e92ca46484ef9cb6d"
          }
        },
        "09e956fdf3e740eebd59561dd88449a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "36fdc7d1d660471db60e8f2566f84526": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1caeb0e0d5e44e0b905b587501de578a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "20125814f4a14c2e92ca46484ef9cb6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fNxB0trzaP8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e45b7c6-0657-4bf1-8268-4b619efa2970"
      },
      "source": [
        "# mount google drive to colab for dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/DL_PROJECT')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fexperimentsandconfigs%20https%3a%2f%2fwww.googleapis.com%2fauth%2fphotos.native&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/1AX4XfWgwHiO1PeNAwvjMzqYTOsRH9aoXWxHLeVtVaDGRhcdqesPWFadE8_E\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MRn4QyBpUM7",
        "outputId": "42994b7f-d7ed-45a1-f7d6-4e72a7849a52"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.6 MB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 636 kB 50.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3 MB 38.4 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 895 kB 26.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S86eNBVm5pWj",
        "outputId": "fbdac465-9cf2-4cad-88ce-ff2d820746aa"
      },
      "source": [
        "!pip install jsonlines"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading jsonlines-2.0.0-py3-none-any.whl (6.3 kB)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bzMRJWSFqAk"
      },
      "source": [
        "'''\n",
        "tokenize all of the sentences and map the tokens to their word IDs.\n",
        "'''\n",
        "def tokenize(sequences):\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every caption...\n",
        "    for seq in sequences:\n",
        "        '''\n",
        "        `encode_plus` will:\n",
        "          (1) Tokenize the caption.\n",
        "          (2) Prepend the `[CLS]` token to the start.\n",
        "          (3) Append the `[SEP]` token to the end.\n",
        "          (4) Map tokens to their IDs.\n",
        "          (5) Pad or truncate the sentence to `max_length`\n",
        "          (6) Create attention masks for [PAD] tokens.\n",
        "        '''\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                            seq,                       # Sentence to encode.\n",
        "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                            max_length = 48,           # Pad & truncate all sentences.\n",
        "                            truncation=True,\n",
        "                            pad_to_max_length = True,\n",
        "                            return_attention_mask = True,   # Construct attn. masks.\n",
        "                            return_tensors = 'pt',      # Return pytorch tensors.\n",
        "                       )\n",
        "\n",
        "        # Add the encoded sentence to the list.    \n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "        # And its attention mask (simply differentiates padding from non-padding).\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    # Convert the lists into tensors.\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    \n",
        "    \n",
        "    return input_ids, attention_masks"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAHkINaHh1fD"
      },
      "source": [
        "from PIL import Image\n",
        "import json\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from torch.utils import data\n",
        "from torchvision import transforms, datasets, models\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict\n",
        "\n",
        "#Load the BERT tokenizer.\n",
        "import transformers\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "'''\n",
        "Dataloader for creating predictions.csv\n",
        "Returns (Image, Captions, Input_id, Attention_mask and ImageName)\n",
        "'''\n",
        "class mytestdataset():    \n",
        "\n",
        "    def __init__(self, classification_list, name):\n",
        "\n",
        "        super(mytestdataset).__init__()\n",
        "        \n",
        "        self.X = []\n",
        "        self.Cap = []\n",
        "        self.Imagename = []\n",
        "\n",
        "        with jsonlines.open(classification_list) as f:\n",
        "          f1 = []\n",
        "          for line in f:\n",
        "            f1.append(line)\n",
        "          for line1 in f1:\n",
        "            path =  line1['img']\n",
        "            self.X.append('/content/drive/My Drive/DL_PROJECT/hateful_memes/hateful_memes/'+line1['img'])\n",
        "            self.Cap.append(line1['text'])\n",
        "            self.Imagename.append(line1['id'])\n",
        "            #self.Imagename.append(path.split('/')[1][:-4]) \n",
        "        '''\n",
        "        with open(classification_list, mode = 'r') as f:\n",
        "            f1 = json.dumps(str(f))\n",
        "            f2 = json.loads(f1)\n",
        "            for line in f2.split('\\t'):\n",
        "            #for line in f.split('\\t'):\n",
        "                print(line)\n",
        "                id, path, caption = json.loads(line)['id'],json.loads(line)['img'],json.loads(line)['text']\n",
        "                #line = dict(list)\n",
        "                #path = line[\"img\"] #json.loads(line)[\"img\"]\n",
        "                self.X.append('/content/drive/My Drive/DL_PROJECT/hateful_memes/hateful_memes/'+path)\n",
        "                self.Cap.append(caption)\n",
        "                self.Imagename.append(id)\n",
        "                #self.Imagename.append(path.split('/')[1][:-4])  \n",
        "        '''  \n",
        "        '''\n",
        "        Tokenize all of the captions and map the tokens to their word IDs, and get respective attention masks.\n",
        "        '''\n",
        "        self.input_ids, self.attention_masks = tokenize(self.Cap)\n",
        "        \n",
        "        \n",
        "        \n",
        "        '''\n",
        "        Image Transforms\n",
        "        '''\n",
        "        self.transform = transforms.Compose([   transforms.Resize(256),\n",
        "                                                transforms.CenterCrop(224),\n",
        "                                                transforms.ToTensor(),\n",
        "                                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                                     std=[0.229, 0.224, 0.225])\n",
        "                                            ])\n",
        "         \n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        \n",
        "        \n",
        "        '''\n",
        "        Image\n",
        "        '''\n",
        "        image = self.X[index]\n",
        "                \n",
        "        image = Image.open(image).convert('RGB') #.replace('img/', '')))\n",
        "               \n",
        "        image = self.transform(image)\n",
        "        \n",
        "       \n",
        "        '''\n",
        "        For Captions, Input ids, Attention mask and Imagename\n",
        "        '''\n",
        "        caption = self.Cap[index]\n",
        "        input_id = self.input_ids[index]\n",
        "        attention_masks = self.attention_masks[index]\n",
        "        Imagename = self.Imagename[index]\n",
        "        \n",
        "        return image, caption, input_id, attention_masks, Imagename\n",
        "        \n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "'''\n",
        "Dataloader for Training/Validation with support for Image Captioning model\n",
        "Returns (Image, Caption, Input_id, Attention_mask, Input_id_Captioning_model, Attention_mask_Captioning_model, label)\n",
        "'''\n",
        "'''\n",
        "class mydataset_captioning():    \n",
        "\n",
        "    def __init__(self, classification_list, name):\n",
        "\n",
        "        super(mydataset_captioning).__init__()\n",
        "        \n",
        "        self.X = []\n",
        "        self.true_Cap = []\n",
        "        self.generated_Cap = []\n",
        "        self.Y = []\n",
        "        \n",
        "        with open(classification_list, mode = 'r') as f:\n",
        "\n",
        "            for line in f:\n",
        "                print(line[:-1].split('\\n'))\n",
        "                path, caption, generated_caption, label = line[:-1].split('\\n')\n",
        "\n",
        "                self.X.append('/content/drive/My Drive/DL_PROJECT/hateful_memes/hateful_memes/'+path)\n",
        "                self.true_Cap.append(caption)\n",
        "                self.generated_Cap.append(generated_caption)\n",
        "                self.Y.append(label)\n",
        "      \n",
        "        #Tokenize all of the captions and map the tokens to thier word IDs, and get respective attention masks.\n",
        "\n",
        "        self.input_ids, self.attention_masks = tokenize(self.true_Cap)\n",
        "        \n",
        "        self.input_ids_cap, self.attention_masks_cap = tokenize(self.generated_Cap)\n",
        "\n",
        "\n",
        "        #Image Transforms\n",
        "\n",
        "        \n",
        "        if name in ['valid','test']:\n",
        "            self.transform = transforms.Compose([   transforms.Resize(384),\n",
        "                                                 transforms.CenterCrop(256),\n",
        "                                                transforms.ToTensor(),\n",
        "                                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                                    std=[0.229, 0.224, 0.225])\n",
        "                                                ])\n",
        "        else:\n",
        "            self.transform = transforms.Compose([ transforms.Resize(256),\n",
        "                                                 transforms.RandomCrop(224),\n",
        "                                                transforms.RandomHorizontalFlip(),\n",
        "                                                transforms.ToTensor(),\n",
        "                                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                                    std=[0.229, 0.224, 0.225])\n",
        "                                                                                            ])\n",
        "    \n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "\n",
        "        #For Image and Label\n",
        "\n",
        "        image = self.X[index]\n",
        "                \n",
        "        image = (Image.open(image))\n",
        "               \n",
        "        image = self.transform(image)\n",
        "        \n",
        "        label = float(self.Y[index])\n",
        "\n",
        "        \n",
        "\n",
        "        #For Captions, Input ids and Attention mask\n",
        "\n",
        "        caption = self.true_Cap[index]\n",
        "        input_id = self.input_ids[index]\n",
        "        attention_masks = self.attention_masks[index]\n",
        "            \n",
        "        input_id_cap = self.input_ids_cap[index]\n",
        "        attention_masks_cap = self.attention_masks_cap[index]\n",
        "    \n",
        "            \n",
        "            \n",
        "        return image, caption, input_id, attention_masks, input_id_cap, attention_masks_cap, torch.as_tensor(label).long()\n",
        "        \n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "'''\n",
        "'''\n",
        "Dataloader for Training/Validation\n",
        "Returns (Image, Caption, Input_id, Attention_mask, label)\n",
        "'''\n",
        "class mydataset():    \n",
        "\n",
        "    def __init__(self, classification_list, name):\n",
        "\n",
        "        super(mydataset).__init__()\n",
        "        \n",
        "        self.X = []\n",
        "        self.Cap = []\n",
        "        self.Y = []\n",
        "\n",
        "        with jsonlines.open(classification_list) as f:\n",
        "          f1 = []\n",
        "          for line in f:\n",
        "            f1.append(line)\n",
        "          for line1 in f1:\n",
        "            path =  line1['img']\n",
        "            self.X.append('/content/drive/My Drive/DL_PROJECT/hateful_memes/hateful_memes/'+line1['img'])\n",
        "            self.Cap.append(line1['text'])\n",
        "            self.Y.append(line1['label'])\n",
        "            #self.Imagename.append(path.split('/')[1][:-4]) \n",
        "        '''\n",
        "        with open(classification_list, mode = 'r') as f:\n",
        "            \n",
        "            for line in f:\n",
        "                print(line)\n",
        "                path, caption, label = line[1],line[3],line[2] #line[:-1].split('\\t')\n",
        "\n",
        "                self.X.append('/content/drive/My Drive/DL_PROJECT/hateful_memes/hateful_memes/'+path)\n",
        "                self.Cap.append(caption)\n",
        "                self.Y.append(label)\n",
        "        '''\n",
        "        '''\n",
        "        Tokenize all of the captions and map the tokens to thier word IDs, and get respective attention masks.\n",
        "        '''\n",
        "        self.input_ids, self.attention_masks = tokenize(self.Cap)\n",
        "        \n",
        "        '''\n",
        "        Image Transforms\n",
        "        '''\n",
        "        \n",
        "        if name in ['valid','test']:\n",
        "            self.transform = transforms.Compose([   transforms.Resize(384),\n",
        "                                                 transforms.CenterCrop(256),\n",
        "                                                transforms.ToTensor(),\n",
        "                                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                                    std=[0.229, 0.224, 0.225])\n",
        "                                                ])\n",
        "        else:\n",
        "            self.transform = transforms.Compose([ transforms.Resize(256),\n",
        "                                                 transforms.RandomCrop(224),\n",
        "                                                transforms.RandomHorizontalFlip(),\n",
        "                                                transforms.ToTensor(),\n",
        "                                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                                    std=[0.229, 0.224, 0.225])\n",
        "                                                                                            ])\n",
        "    \n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        \n",
        "        \n",
        "        '''\n",
        "        For Image and Label\n",
        "        '''\n",
        "        image = self.X[index]\n",
        "                \n",
        "        image = Image.open(image).convert('RGB') #.replace('img/', '')))\n",
        "               \n",
        "        image = self.transform(image)\n",
        "        \n",
        "        label = float(self.Y[index])\n",
        "\n",
        "        \n",
        "        '''\n",
        "        For Captions, Input ids and Attention mask\n",
        "        '''\n",
        "        caption = self.Cap[index]\n",
        "        input_id = self.input_ids[index]\n",
        "        attention_masks = self.attention_masks[index]\n",
        "        \n",
        "        return image, caption, input_id, attention_masks, torch.as_tensor(label).long()\n",
        "        \n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.X)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8j_KE9Lh1Ti"
      },
      "source": [
        "######### Load saved model from checkpoint  #########\n",
        "def load(modelpath, model, optimizer, lr_scheduler):\n",
        "    checkpoint = torch.load(modelpath)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    train_loss = checkpoint['Training_Loss_List'] \n",
        "    v_loss = checkpoint['Validation_Loss_List']\n",
        "    v_acc = checkpoint['Validation_Accuracy_List']\n",
        "\n",
        "    epoch = checkpoint['Epoch']\n",
        "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
        "    \n",
        "    return model, optimizer, lr_scheduler, train_loss, v_loss, v_acc, epoch"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNw5nx7lh084"
      },
      "source": [
        "def plot_loss(epochs, train_loss, v_loss, title):\n",
        "    plt.figure(figsize=(8,8))\n",
        "    x = np.arange(1,epochs+2)\n",
        "    plt.plot(x, train_loss, label = 'Training Loss')\n",
        "    plt.plot(x, v_loss, label = 'Validation Loss')\n",
        "    plt.xlabel('Epochs', fontsize =16)\n",
        "    plt.ylabel('Loss', fontsize =16)\n",
        "    plt.title(title,fontsize =16)\n",
        "    plt.legend(fontsize=16)\n",
        "    \n",
        "    \n",
        "def plot_acc(epochs,v_acc):\n",
        "    plt.figure(figsize=(8,8))\n",
        "    x = np.arange(1,epochs+2)\n",
        "    plt.plot(x, v_acc)\n",
        "    plt.xlabel('Epochs', fontsize =16)\n",
        "    plt.ylabel('Validation Accuracy', fontsize =16)\n",
        "    plt.title('Validation Accuracy v/s Epochs',fontsize =16)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUNISXygsX37"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "#         self.batchnorm = nn.BatchNorm1d(num_features=512 * block.expansion)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        # See note [TorchScript super()]\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)        \n",
        "        embedding = x\n",
        "        x = self.fc(embedding)\n",
        "           \n",
        "#         return x\n",
        "        return x, embedding\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "def ResNet50(img_channels=3,num_classes=1000):\n",
        "     \"\"\"ResNet-50 model from\n",
        "     `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "     Args:\n",
        "         pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "         progress (bool): If True, displays a progress bar of the download to stderr\n",
        "     \"\"\"\n",
        "     return ResNet(BasicBlock, [3, 4, 6, 3], img_channels,num_classes)\n",
        "\n",
        "'''\n",
        "def resnext101_32x8d(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNeXt-101 32x8d model from\n",
        "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['groups'] = 32\n",
        "    kwargs['width_per_group'] = 8\n",
        "    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\n",
        "                   pretrained, progress, **kwargs)\n",
        "    \n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "'''\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckJO12iNuNBC"
      },
      "source": [
        "'''\n",
        "For BERT\n",
        "'''\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHfFe7WDaP8k"
      },
      "source": [
        "**Device**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFeNpEoAaP8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989e27a2-5e12-48b1-f949-e07c37fcbb75"
      },
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "#gpu_ids = [7,6]\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9tV3N_haP8l"
      },
      "source": [
        "**Dataloading Scheme**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "T1L722dQ5QXM",
        "outputId": "37a1f0bb-8298-47ba-8665-78d0cfe46508"
      },
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/DL_PROJECT'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBPb5OqiaP8l"
      },
      "source": [
        "trainlist = '/content/drive/My Drive/DL_PROJECT/hateful_memes/hateful_memes/train.jsonl'\n",
        "validlist = '/content/drive/My Drive/DL_PROJECT/hateful_memes/hateful_memes/dev_seen.jsonl'"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYE7ZWHgaP8l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ebe92d2-ce41-40f0-e276-94da31db5cfc"
      },
      "source": [
        "'''\n",
        "Train Dataloader\n",
        "''' \n",
        "#nw = min([os.cpu_count() // world_size, batch_size if batch_size > 1 else 0, max_worker])\n",
        "#prime_dict = create_prime_dict(trainlist)\n",
        "\n",
        "train_dataset = mydataset(trainlist,name='train')   #mydataset_captioning(trainlist,name='train')          \n",
        "train_dataloader = data.DataLoader(train_dataset, shuffle= True, batch_size = 8, num_workers=16,pin_memory=True)\n",
        "\n",
        "\n",
        "'''\n",
        "Validation Dataloader\n",
        "''' \n",
        "validation_dataset = mydataset(validlist, name='valid')  #mydataset_captioning(validlist, name='valid')         \n",
        "validation_dataloader = data.DataLoader(validation_dataset, shuffle=False, batch_size = 8, num_workers=16,pin_memory=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFqdxztfaP8m"
      },
      "source": [
        "**Model Definition**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxRnFreKaP8n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "78b481d0-c6c6-423b-bb1e-7f01aca30ba5"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "    \n",
        "'''\n",
        "Model1 ResNet50\n",
        "'''\n",
        "Image_model = ResNet(Bottleneck, [3, 4, 6, 3], num_classes = 2)\n",
        "Image_model = nn.DataParallel(Image_model).to(device)\n",
        "\n",
        "\n",
        "'''\n",
        "#Load saved model from checkpoint\n",
        "\n",
        "model1_name = 'Resnet50'\n",
        "model1_path = './saved_model_checkpoints/'+model1_name\n",
        "\n",
        "checkpoint1 = torch.load(model1_path)\n",
        "Image_model.load_state_dict(checkpoint1['model_state_dict'])\n",
        "\n",
        "Image_model.to(device)\n",
        "'''\n",
        "'''\n",
        "Model1 ResNeXt101_32x8d\n",
        "\n",
        "#Image_model = ResNet(Bottleneck, [3, 4, 6, 3], num_classes = 2)\n",
        "Image_model = resnext101_32x8d()\n",
        "\n",
        "Image_model.fc = nn.Sequential(\n",
        "    nn.Linear(Image_model.fc.in_features, 2)\n",
        "    )\n",
        "\n",
        "Image_model = nn.DataParallel(Image_model).to(device)\n",
        "\n",
        "\n",
        "\n",
        "#Load saved model from checkpoint\n",
        "\n",
        "#model1_name = 'ResneXt101_32x8d'\n",
        "#model1_path = './saved_model_checkpoints/'+model1_name\n",
        "\n",
        "#checkpoint1 = torch.load(model1_path)\n",
        "#Image_model.load_state_dict(checkpoint1['model_state_dict'])\n",
        "#Image_model.to(device)\n",
        "'''"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nModel1 ResNeXt101_32x8d\\n\\n#Image_model = ResNet(Bottleneck, [3, 4, 6, 3], num_classes = 2)\\nImage_model = resnext101_32x8d()\\n\\nImage_model.fc = nn.Sequential(\\n    nn.Linear(Image_model.fc.in_features, 2)\\n    )\\n\\nImage_model = nn.DataParallel(Image_model).to(device)\\n\\n\\n\\n#Load saved model from checkpoint\\n\\n#model1_name = 'ResneXt101_32x8d'\\n#model1_path = './saved_model_checkpoints/'+model1_name\\n\\n#checkpoint1 = torch.load(model1_path)\\n#Image_model.load_state_dict(checkpoint1['model_state_dict'])\\n#Image_model.to(device)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AyJYW6haP8n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "6caf4a0ec3b8494a961b3dbf8a738ea6",
            "112642ddaef74be18e732830d79af247",
            "f3fa88bd8335417bb1423f5158473f22",
            "46f3761f2b5e42d3bdd44deed4259ab8",
            "09e956fdf3e740eebd59561dd88449a9",
            "36fdc7d1d660471db60e8f2566f84526",
            "1caeb0e0d5e44e0b905b587501de578a",
            "20125814f4a14c2e92ca46484ef9cb6d"
          ]
        },
        "outputId": "3220401d-1042-4f1e-c292-3efb679403f1"
      },
      "source": [
        "'''\n",
        "Model 2 BERT\n",
        "\n",
        "Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n",
        "''' \n",
        "\n",
        "Text_model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", \n",
        "    num_labels = 2,   \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = True\n",
        ")\n",
        "\n",
        "Text_model = nn.DataParallel(Text_model).to(device)\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6caf4a0ec3b8494a961b3dbf8a738ea6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJUDtZO925Ky"
      },
      "source": [
        "model2_name = 'BERT_basic'\n",
        "model_path = '/content/drive/My Drive/DL_PROJECT/' +model2_name"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "CSPKHEz7l1ol",
        "outputId": "d5777b8c-44f7-4b69-c0de-f981d091c8a3"
      },
      "source": [
        "'''\n",
        "Load saved model from checkpoint\n",
        "'''\n",
        "checkpoint2 = torch.load(model2_path)\n",
        "Text_model.load_state_dict(checkpoint2['model_state_dict'])\n",
        "\n",
        "Text_model.to(device)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-8dfe850adc9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mLoad\u001b[0m \u001b[0msaved\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m '''\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcheckpoint2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mText_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/DL_PROJECT/saved_model_checkpoints/BERT_basic'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3VLQHnuaP8o"
      },
      "source": [
        "'''\n",
        "Fusion\n",
        "'''\n",
        "'''\n",
        "class FusionNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_classes, drop_prob = 0.1):\n",
        "        super(FusionNet, self).__init__()\n",
        "        \n",
        "        self.pooler = nn.Linear(in_features=768, out_features=768)\n",
        "        \n",
        "        self.concat = nn.Linear(in_features=768+2048, out_features= 512)\n",
        "                \n",
        "        self.classify = nn.Linear(in_features = 512, out_features = num_classes)\n",
        "        \n",
        "        \n",
        "    def forward(self, text_features, image_features):\n",
        "\n",
        "        x =(text_features+image_features)/2\n",
        "\n",
        "        return x\n",
        "\n",
        "'''\n",
        "#Fusion - Image Features, Text Features and Captions generated by our Captioning model\n",
        "\n",
        "class FusionNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_classes, drop_prob = 0.1):\n",
        "        super(FusionNet, self).__init__()\n",
        "        \n",
        "        self.concat = nn.Linear(in_features=768+2048, out_features= 512)\n",
        "        \n",
        "        self.bn = nn.BatchNorm1d(512)\n",
        "        self.bn1 = nn.BatchNorm1d(768)\n",
        "        self.bn2 = nn.BatchNorm1d(2048)\n",
        "        #self.bn3 = nn.BatchNorm1d(768)\n",
        "\n",
        "        \n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        self.classify = nn.Linear(in_features = 512, out_features = num_classes)\n",
        "        \n",
        "        \n",
        "    def forward(self, text_features, image_features): #, caption_features):\n",
        "\n",
        "        text_features = self.bn1(text_features)\n",
        "        image_features = self.bn2(image_features)\n",
        "        #caption_features = self.bn3(caption_features)\n",
        "\n",
        "        fused_input =  torch.cat((text_features, image_features), dim=1)\n",
        "        \n",
        "        x = self.concat(fused_input)\n",
        "        x = F.relu(self.bn(x))        \n",
        "        \n",
        "        x = F.relu(self.classify(x)) \n",
        "\n",
        "        return x\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GT7YNizfH9r"
      },
      "source": [
        "train_loss= []\n",
        "v_loss = []\n",
        "v_acc = []\n",
        "\n",
        "def train(image_model,text_model,fusion_model,data_loader,test_loader,criterion,optimizer, lr_scheduler, modelpath, writer, device, epochs):\n",
        "    \n",
        "    fusion_model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        avg_loss = 0.0\n",
        "                \n",
        "        \n",
        "        for batch_num, (feats, captions, input_id, attention_masks, target) in enumerate(data_loader):\n",
        "            \n",
        "            feats, target = feats.to(device), target.to(device)\n",
        "            input_ids, attention_masks = input_id.to(device), attention_masks.to(device)\n",
        "               \n",
        "            '''\n",
        "            Compute ResNet Features\n",
        "            '''\n",
        "            out, image_features = image_model(feats) \n",
        "           \n",
        "                            \n",
        "            '''\n",
        "            Compute BERT Features\n",
        "            Take hidden state corresponding to [CLS] token from the final transformer\n",
        "            '''\n",
        "            output_dictionary = text_model(input_ids, \n",
        "                                           token_type_ids=None, \n",
        "                                           attention_mask=attention_masks, \n",
        "                                           labels=target,\n",
        "                                           return_dict = True)\n",
        "            \n",
        "            text_features = output_dictionary.hidden_states[12][:,0,:]\n",
        "            \n",
        "            \n",
        "\n",
        "            \n",
        "            \n",
        "            '''\n",
        "            Compute Classification Output and loss from Fusion model\n",
        "            '''\n",
        "            output = fusion_model(text_features, image_features)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            \n",
        "                   \n",
        "            '''\n",
        "            Take Step\n",
        "            '''                    \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            avg_loss += loss.item()\n",
        "\n",
        "\n",
        "#             '''\n",
        "#             linear_schedule_with_warmup take step after each batch\n",
        "#             '''\n",
        "#             lr_scheduler.step()\n",
        "            \n",
        "                        \n",
        "#             if batch_num % 100 == 99:\n",
        "#                 print('loss', avg_loss/100)\n",
        "                \n",
        "            del feats\n",
        "            del captions\n",
        "            del input_ids\n",
        "            del attention_masks\n",
        "            del target\n",
        "            del loss\n",
        "            \n",
        "            \n",
        "        training_loss = avg_loss/len(data_loader)\n",
        "       \n",
        "        print('Epoch: ', epoch+1)            \n",
        "        print('training loss = ', training_loss)\n",
        "        train_loss.append(training_loss)\n",
        "\n",
        "        \n",
        "        \n",
        "        '''\n",
        "        Learning rate scheduler\n",
        "        '''\n",
        "        lr_scheduler.step()\n",
        "            \n",
        "            \n",
        "        '''\n",
        "        Check performance on validation set after an Epoch\n",
        "        '''\n",
        "        \n",
        "        valid_loss, top1_acc= test_classify(image_model, text_model, fusion_model, test_loader, criterion, device)\n",
        "        print('Validation Loss: {:.4f}\\tTop 1 Validation Accuracy: {:.4f}'.format(valid_loss, top1_acc))\n",
        "        v_loss.append(valid_loss)\n",
        "        v_acc.append(top1_acc)\n",
        "\n",
        "        '''\n",
        "        Logs\n",
        "        '''\n",
        "        writer.add_scalar(\"Loss/train\", training_loss, epoch)            \n",
        "        writer.add_scalar('Loss/Validation', valid_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/Validation', top1_acc, epoch)\n",
        "\n",
        "        \n",
        "        #save fusion model checkpoint after every epoch\n",
        "        '''\n",
        "        torch.save({\n",
        "            'model_state_dict': fusion_model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'Training_Loss_List':train_loss,\n",
        "            'Validation_Loss_List':v_loss,\n",
        "            'Validation_Accuracy_List': v_acc,\n",
        "            'Epoch':epoch,\n",
        "            'lr_scheduler': lr_scheduler.state_dict() \n",
        "\n",
        "            }, modelpath)\n",
        "        '''\n",
        "\n",
        "        \n",
        "'''\n",
        "Returns Loss and top1 accuracy on test/validation set\n",
        "'''\n",
        "def test_classify(image_model, text_model, fusion_model, test_loader, criterion, device):\n",
        "    fusion_model.eval()\n",
        "    test_loss = []\n",
        "    top1_accuracy = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_num, (feats, captions, input_id, attention_masks, target) in enumerate(test_loader):\n",
        "        \n",
        "        feats, target = feats.to(device), target.to(device)\n",
        "        input_ids, attention_masks = input_id.to(device), attention_masks.to(device)\n",
        "        \n",
        "        \n",
        "        '''\n",
        "        Compute ResNet Features\n",
        "        '''\n",
        "        out, image_features = image_model(feats) \n",
        "\n",
        "        \n",
        "        \n",
        "        '''\n",
        "        Compute BERT Features\n",
        "        '''\n",
        "        output_dictionary = text_model(input_ids, \n",
        "                                       token_type_ids=None, \n",
        "                                       attention_mask=attention_masks, \n",
        "                                       labels=target,\n",
        "                                       return_dict = True)\n",
        "\n",
        "        text_features = output_dictionary.hidden_states[12][:,0,:]\n",
        "        \n",
        "        \n",
        "        '''\n",
        "        Compute Classification Output and loss from Fusion model\n",
        "        '''\n",
        "        output = fusion_model(text_features, image_features)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "            \n",
        "        test_loss.extend([loss.item()]*feats.size()[0])\n",
        "        \n",
        "        \n",
        "        \n",
        "        '''\n",
        "        Prediction\n",
        "        '''\n",
        "        \n",
        "        predictions = F.softmax(output, dim=1)\n",
        "        \n",
        "        _, top1_pred_labels = torch.max(predictions,1)\n",
        "        top1_pred_labels = top1_pred_labels.view(-1)\n",
        "        \n",
        "        top1_accuracy += torch.sum(torch.eq(top1_pred_labels, target)).item()\n",
        "        \n",
        "        \n",
        "        total += len(target)\n",
        "        \n",
        "        del feats\n",
        "        del captions\n",
        "        del input_ids\n",
        "        del attention_masks\n",
        "        del target\n",
        "        del loss\n",
        "            \n",
        "    fusion_model.train()\n",
        "    return np.mean(test_loss), top1_accuracy/total"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85pRXktUaP8o"
      },
      "source": [
        "Fusion_model = FusionNet(num_classes = 2 , drop_prob = 0.1)\n",
        "Fusion_model = nn.DataParallel(Fusion_model).to(device)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agClo63KaP8p"
      },
      "source": [
        "model_name = 'Late Fusion Model'\n",
        "model_path = '/content/drive/My Drive/DL_PROJECT/' +model_name"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I08ZjXbr0TMT"
      },
      "source": [
        "**Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zLThvRyzU0E"
      },
      "source": [
        "'''\n",
        "Loss Function\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "'''\n",
        "Optimizer\n",
        "'''\n",
        "optimizer = torch.optim.SGD(Fusion_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
        "# optimizer = AdamW(Fusion_model.parameters(), lr = 2e-3, eps = 1e-8)\n",
        "\n",
        "\n",
        "'''\n",
        "Number of training epochs.\n",
        "'''\n",
        "num_Epochs = 2\n",
        "\n",
        "\n",
        "# '''\n",
        "# OneCycleLR\n",
        "# '''\n",
        "# max_lr = 0.05\n",
        "# lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, total_steps=None, epochs=num_Epochs, steps_per_epoch=len(train_dataloader), pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0, last_epoch=-1)\n",
        "\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 4, gamma = 0.1)\n",
        "\n",
        "# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oZqTmhEXfb9R",
        "outputId": "0c2c2027-0947-4b58-c014-46cec76dcf5f"
      },
      "source": [
        "'''\n",
        "Load saved model from checkpoint\n",
        "'''\n",
        "#Fusion_model, optimizer, lr_scheduler, train_loss, v_loss, v_acc, epoch = load(model_path, Fusion_model, optimizer, lr_scheduler)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nLoad saved model from checkpoint\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRynP2D3VVIT",
        "outputId": "067f464c-773c-423d-c3dd-643ab729d2ee"
      },
      "source": [
        "#torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "torch.cuda.empty_cache()\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter(model_name)\n",
        "\n",
        "train(Image_model, Text_model, Fusion_model, train_dataloader, validation_dataloader, criterion, optimizer, lr_scheduler, model_path, writer, device, epochs = num_Epochs)\n",
        "\n",
        "writer.flush()\n",
        "writer.close()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1\n",
            "training loss =  0.6931471824645996\n",
            "Validation Loss: 0.6931\tTop 1 Validation Accuracy: 0.5060\n",
            "Epoch:  2\n",
            "training loss =  0.6931471824645996\n",
            "Validation Loss: 0.6931\tTop 1 Validation Accuracy: 0.5060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZY-h3UtaP8q"
      },
      "source": [
        "**Evaluate**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzTSBs4SaP8q"
      },
      "source": [
        "**Predict on Test and generate output.csv**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQM4UyDkaP8r"
      },
      "source": [
        "**Test Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHDvADPBaP8r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c7a76f7-694b-48ff-bd58-832c998ed031"
      },
      "source": [
        "testlist = '/content/drive/My Drive/DL_PROJECT/hateful_memes/hateful_memes/test_seen.jsonl'\n",
        "\n",
        "test_dataset = mytestdataset(testlist, name='test')          \n",
        "test_dataloader = data.DataLoader(test_dataset, shuffle= False, batch_size = 32, num_workers=8,pin_memory=True)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0x2zuCGiMRj",
        "outputId": "855a22d2-f8e3-46e5-b63a-ca2595993200"
      },
      "source": [
        "test_classify(Image_model, Text_model, Fusion_model, validation_dataloader, criterion, device)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6931471824645996, 0.506)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPoGZ1L0aP8r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "731c97e5-bcd4-40aa-e62b-377e0b75442b"
      },
      "source": [
        "from predict import predict\n",
        "predict(image_model, text_model, fusion_model, test_dataloader, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-138-b49e6001bd20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpredict\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfusion_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'predict'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}